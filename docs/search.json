[
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "My Portfolio",
    "section": "",
    "text": "SAP SD\nSAP TM\nC(Programming Language)\nPython\nR Language\nMicrosoft Excel"
  },
  {
    "objectID": "education.html",
    "href": "education.html",
    "title": "My Portfolio",
    "section": "",
    "text": "George Mason University Fairfax, VA\n\nMasterâ€™s degree: Data Modeling/Warehousing and Database Administration\n\nSRKR Engineering College Bhimavaram, India\n\nBTech - Bachelor of Technology: Electrical, Electronics and Communications Engineering"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Hi ğŸ‘‹, Iâ€™m Sriya Pothula and Iâ€™m currently pursuing my Masterâ€™s in Data Analytics Engineering at George Mason University.\nI am a driven SAP S&M Analyst with a strong background in business systems, process optimization, and data analytics. Leveraging SAP Sales and Distribution (S&D) module expertise, adept at analyzing complex sales and marketing data to drive strategic decision-making and improve operational efficiency. Eager to learn new things and work on competitive tasks."
  },
  {
    "objectID": "Shinycode.html",
    "href": "Shinycode.html",
    "title": "Shiny Code",
    "section": "",
    "text": "library(shiny)\nui = fluidPage(\nÂ  titlePanel(â€œAbalone Age Predictionâ€),\nÂ  sidebarLayout(\nÂ Â Â  sidebarPanel(\nÂ Â Â Â Â  numericInput(â€œlengthâ€, â€œLengthâ€, value = 0.5),\nÂ Â Â Â Â  numericInput(â€œdiameterâ€, â€œDiameterâ€, value = 0.4),\nÂ Â Â Â Â  numericInput(â€œheightâ€, â€œHeightâ€, value = 0.1),\nÂ Â Â Â Â  numericInput(â€œwhole_weightâ€, â€œWhole Weightâ€, value = 0.5),\nÂ Â Â Â Â  numericInput(â€œshucked_weightâ€, â€œShucked Weightâ€, value = 0.2),\nÂ Â Â Â Â  numericInput(â€œviscera_weightâ€, â€œViscera Weightâ€, value = 0.1),\nÂ Â Â Â Â  numericInput(â€œshell_weightâ€, â€œShell Weightâ€, value = 0.15),\nÂ Â Â Â Â  actionButton(â€œpredictâ€, â€œPredict Ageâ€)\nÂ Â Â  ),\nÂ Â Â  mainPanel(\nÂ Â Â Â Â  textOutput(â€œage_predictionâ€)\nÂ Â Â  )\nÂ  )\n)\nserver = function(input, output) {\nÂ  observeEvent(input$predict, {\nÂ Â Â  new_data = data.frame(\nÂ Â Â Â Â  Length = input$length,\nÂ Â Â Â Â  Diameter = input$diameter,\nÂ Â Â Â Â  Height = input$height,\nÂ Â Â Â Â  Whole_weight = input$whole_weight,\nÂ Â Â Â Â  Shucked_weight = input$shucked_weight,\nÂ Â Â Â Â  Viscera_weight = input$viscera_weight,\nÂ Â Â Â Â  Shell_weight = input$shell_weight\nÂ Â Â  )\nÂ Â Â  predicted_age &lt;- predict(models$rf, new_data)\nÂ Â Â  output$age_prediction &lt;- renderText({\nÂ Â Â Â Â  paste(â€œPredicted age:â€, round(predicted_age))\nÂ Â Â  })\nÂ  })\n}\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Shinycode.html#abalone-age-predictor",
    "href": "Shinycode.html#abalone-age-predictor",
    "title": "Shiny Code",
    "section": "",
    "text": "library(shiny)\nui = fluidPage(\nÂ  titlePanel(â€œAbalone Age Predictionâ€),\nÂ  sidebarLayout(\nÂ Â Â  sidebarPanel(\nÂ Â Â Â Â  numericInput(â€œlengthâ€, â€œLengthâ€, value = 0.5),\nÂ Â Â Â Â  numericInput(â€œdiameterâ€, â€œDiameterâ€, value = 0.4),\nÂ Â Â Â Â  numericInput(â€œheightâ€, â€œHeightâ€, value = 0.1),\nÂ Â Â Â Â  numericInput(â€œwhole_weightâ€, â€œWhole Weightâ€, value = 0.5),\nÂ Â Â Â Â  numericInput(â€œshucked_weightâ€, â€œShucked Weightâ€, value = 0.2),\nÂ Â Â Â Â  numericInput(â€œviscera_weightâ€, â€œViscera Weightâ€, value = 0.1),\nÂ Â Â Â Â  numericInput(â€œshell_weightâ€, â€œShell Weightâ€, value = 0.15),\nÂ Â Â Â Â  actionButton(â€œpredictâ€, â€œPredict Ageâ€)\nÂ Â Â  ),\nÂ Â Â  mainPanel(\nÂ Â Â Â Â  textOutput(â€œage_predictionâ€)\nÂ Â Â  )\nÂ  )\n)\nserver = function(input, output) {\nÂ  observeEvent(input$predict, {\nÂ Â Â  new_data = data.frame(\nÂ Â Â Â Â  Length = input$length,\nÂ Â Â Â Â  Diameter = input$diameter,\nÂ Â Â Â Â  Height = input$height,\nÂ Â Â Â Â  Whole_weight = input$whole_weight,\nÂ Â Â Â Â  Shucked_weight = input$shucked_weight,\nÂ Â Â Â Â  Viscera_weight = input$viscera_weight,\nÂ Â Â Â Â  Shell_weight = input$shell_weight\nÂ Â Â  )\nÂ Â Â  predicted_age &lt;- predict(models$rf, new_data)\nÂ Â Â  output$age_prediction &lt;- renderText({\nÂ Â Â Â Â  paste(â€œPredicted age:â€, round(predicted_age))\nÂ Â Â  })\nÂ  })\n}\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "Final_Project.html",
    "href": "Final_Project.html",
    "title": "Final Project",
    "section": "",
    "text": "Group 9"
  },
  {
    "objectID": "Final_Project.html#predicting-the-age-of-abalone",
    "href": "Final_Project.html#predicting-the-age-of-abalone",
    "title": "Final Project",
    "section": "Predicting the Age of Abalone:",
    "text": "Predicting the Age of Abalone:\nLet us start with relationship between features ( correlation coefficient matrix) and the relationship between the visual features. The objective of the data visualization step in our research is to conduct a comprehensive exploratory data analysis (EDA) to understand the relationships between various physical measurements of abalones and their sex classification (Female, Male, Infant). The visualization aims to uncover underlying patterns, associations, and distributions within the dataset, which are crucial for informing subsequent data processing and predictive modeling stages.\nWe employ the GGally package in R, specifically the ggpairs function, to generate a pairs plot. This function is highly effective for visualizing pairwise relationships and distributions across multiple dimensions of a dataset.\n\n\nCode\nlibrary(tidyverse)\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(GGally)\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nCode\nggpairs(abalone, aes(colour = Sex, alpha = 0.8), title=\"Pairs plot for abalone dataset\") + \n  theme_grey(base_size = 8)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFrom the above plots, Variables such as Length, Diameter, and Whole_weight show strong positive correlations with each other across all sex categories. This is expected as larger abalones generally have greater dimensions and weights. Length diameter and height have similar distributions across sexes but with slight variations in median and variability, particularly between infants and adults. Whole weight and Shucked weight Show significant differences among the sexes, suggesting that these features are good discriminators for sex classification.\nThe presence of â€œ***â€ next to a correlation value suggests a very strong statistical significance for that correlation."
  },
  {
    "objectID": "Final_Project.html#data-visualization-and-statistical-analysis",
    "href": "Final_Project.html#data-visualization-and-statistical-analysis",
    "title": "Final Project",
    "section": "Data Visualization and Statistical Analysis",
    "text": "Data Visualization and Statistical Analysis\nIn the initial phase of our study, we employ ggplot2 from the tidyverse package to create visual representations of the abalone dataset. A histogram details the distribution of the â€˜Ringsâ€™ variable, which serves as a proxy for age, while a boxplot is used to compare this distribution across different sex categories, providing insights into central tendencies and variability. Additionally, a pairs plot generated using the base R function pairs visually assesses the relationships among key continuous variables such as Length, Diameter, Height, and Whole_weight. This visualization is instrumental in identifying correlations and potential multicollinearity among predictors. Furthermore, an Analysis of Variance (ANOVA) tests for significant differences in the mean number of rings across sex groups, helping determine if sex significantly influences abalone age.\n\n\nCode\nlibrary(tidyverse)\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nCode\nlibrary(caret)\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\nlibrary(corrplot)\n\n\ncorrplot 0.92 loaded\n\n\nCode\nggplot(abalone, aes(x = Rings)) + \n  geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\") +\n  labs(title = \"Distribution of Rings\")\n\n\n\n\n\n\nInterpretation:\nThe histogram shows a unimodal distribution with a strong peak around 9 to 10 rings, suggesting that the majority of abalones in the dataset are around 9 to 10 years old. The distribution is slightly right-skewed, indicating a tail of older abalones with more rings. Few abalones have very high ring counts, as seen from the declining frequency past 15 rings, which is typical for biological data where fewer individuals reach older ages. This visualization provides a clear view of the age structure within the abalone population studied, which is crucial for biological and ecological analysis, potentially guiding conservation and harvesting strategies.\n\n\nCode\nggplot(abalone, aes(x = Sex, y = Rings)) + \n  geom_boxplot(fill = \"cyan\", color = \"black\") +\n  labs(title = \"Rings Distribution by Sex\")\n\n\n\n\n\n\n\nCode\npairs(~Length + Diameter + Height + Whole_weight, data = abalone, \n      main = \"Pairwise Relationships\", pch = 20)\n\n\n\n\n\n\n\nInterpretation:\nThe â€œPairwise Relationshipsâ€ plot displays the correlation between several physical measurements of abalones: Length, Diameter, Height, and Whole_weight. Each plot demonstrates a positive correlation among these variables, particularly between Length and Diameter, and Length and Whole_weight, which show strong linear relationships. This indicates that as abalones grow longer, they also tend to increase proportionally in diameter and overall weight. The relationship between Height and the other variables, while still positive, appears weaker and more dispersed, suggesting that height may vary more independently compared to length or diameter. Overall, these relationships are expected in biological organisms, where physical dimensions often scale together, but the varying degrees of correlation may also hint at different growth patterns or environmental adaptations specific to the species.\nFurthermore, an Analysis of Variance (ANOVA) tests for significant differences in the mean number of rings across sex groups, helping determine if sex significantly influences abalone age.\n\n\nCode\nanova_result &lt;- aov(Rings ~ Sex, data = abalone)\nsummary(anova_result)\n\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSex            2   8381    4191   499.3 &lt;2e-16 ***\nResiduals   4174  35030       8                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpretation:\nThe output from the ANOVA (Analysis of Variance) test indicates a significant effect of the variable â€˜Sexâ€™ on the number of rings in abalones, which are used as a proxy for age. With a very high F-value of 499.3 and an extremely low p-value (less than 2e-16), we reject the null hypothesis that there are no differences in mean number of rings across the sex categories of abalones. This suggests that the average number of rings, and hence the age distribution, differs statistically significantly between the sexes. The degrees of freedom for the â€˜Sexâ€™ group is 2, reflecting the three levels compared (female, male, and infant), and the residuals have 4174 degrees of freedom, indicating the sample size minus the number of groups. This strong statistical evidence supports that sex is a significant factor influencing the age as measured by rings in abalones.\n\n\nCorrelation Analysis and Machine Learning Modeling:\nCorrelation analysis is conducted using the cor function to establish a correlation matrix for numerical variables, quantifying the degree of linear relationships between them. This analysis aids in feature selection and informs subsequent modeling strategies. The study progresses by employing machine learning techniques where a Random Forest model is trained using the randomForest package to ascertain variable importance, which guides the refinement of our predictive models. Subsequent models, including Support Vector Machines (SVM), Gradient Boosting Machines (GBM), and Linear Regression, are trained and compared using the caret package to evaluate their performance based on Root Mean Square Error (RMSE) and R^2 metrics. These steps encapsulate our approach to harnessing advanced analytics to predict the age of abalones more accurately, enhancing our understanding of this speciesâ€™ growth patterns.\n\n\nCode\nabalone$Sex &lt;- as.factor(abalone$Sex)\ncorrelations &lt;- cor(abalone[,-1])\ncorrelations\n\n\n                  Length  Diameter    Height Whole_weight Shucked_weight\nLength         1.0000000 0.9868116 0.8275536    0.9252612      0.8979137\nDiameter       0.9868116 1.0000000 0.8336837    0.9254521      0.8931625\nHeight         0.8275536 0.8336837 1.0000000    0.8192208      0.7749723\nWhole_weight   0.9252612 0.9254521 0.8192208    1.0000000      0.9694055\nShucked_weight 0.8979137 0.8931625 0.7749723    0.9694055      1.0000000\nViscera_weight 0.9030177 0.8997244 0.7983193    0.9663751      0.9319613\nShell_weight   0.8977056 0.9053298 0.8173380    0.9553554      0.8826171\nRings          0.5567196 0.5746599 0.5574673    0.5403897      0.4208837\n               Viscera_weight Shell_weight     Rings\nLength              0.9030177    0.8977056 0.5567196\nDiameter            0.8997244    0.9053298 0.5746599\nHeight              0.7983193    0.8173380 0.5574673\nWhole_weight        0.9663751    0.9553554 0.5403897\nShucked_weight      0.9319613    0.8826171 0.4208837\nViscera_weight      1.0000000    0.9076563 0.5038192\nShell_weight        0.9076563    1.0000000 0.6275740\nRings               0.5038192    0.6275740 1.0000000\n\n\n\n\nInterpretation:\nThe variable importance plots from a random forest model reveal key predictors for estimating the age of abalones, measured by the number of rings. In the left plot, which shows percentage increase in mean squared error (%IncMSE), â€˜Shucked_weightâ€™ emerges as the most critical predictor, followed by â€˜Sexâ€™ and â€˜Shell_weightâ€™, indicating that model accuracy heavily relies on these variables. The right plot, displaying increase in node purity (IncNodePurity), also highlights â€˜Shell_weightâ€™ as the top variable, with â€˜Whole_weightâ€™ and â€˜Shucked_weightâ€™ also showing significant influence. These metrics suggest that factors related to the physical composition of abalones, particularly the weights of different body parts, along with the sex, play crucial roles in accurately determining their age, with these variables significantly affecting the modelâ€™s predictive accuracy and ability to create homogeneous groups.\n\n\nImplementing different models:\nThe dataset is split into training and testing sets, 80-20 respectively. The training control is prepared for 10-fold cross-validation, a technique that improves the reliability of model evaluation by training and testing the models on ten distinct data subsets.\nRandom Forest: A regression model utilizing random forests is trained. Multiple decision trees are used in this ensemble model to reduce overfitting and increase prediction accuracy.\n\n\nCode\nset.seed(12)\nmodel &lt;- randomForest(Rings ~ ., data = abalone, importance = TRUE)\nvarImpPlot(model)\n\n\n\n\n\nCode\nabalone_no_sex &lt;- abalone[, !names(abalone) %in% c(\"Sex\")]\nindex &lt;- createDataPartition(abalone_no_sex$Rings, p = 0.8, list = TRUE)\ntrain_data &lt;- abalone_no_sex[index$Resample1,]\ntest_data &lt;- abalone_no_sex[-index$Resample1,]\ntrain_control &lt;- trainControl(\n  method = \"cv\", \n  number = 10   \n)\nmodels &lt;- list()\nmodels$rf &lt;- train(Rings ~ ., data = abalone_no_sex, method = \"rf\", trControl = train_control)\n\n\nSupport Vector Machine (SVM): Data scaling is used to standardize feature scales after an SVM model with a radial basis function kernel has been trained.\n\n\nCode\nmodels$svm &lt;- train(Rings ~ ., data = abalone_no_sex, method = \"svmRadial\", trControl = train_control, preProcess = \"scale\")\n\n\nGradient Boosting Machine (GBM): After training, a GBM model creates trees one at a time, with each new tree aiding in the correction of mistakes produced by trees that have already been trained.\n\n\nCode\n# GBM\nmodels$gbm &lt;- train(Rings ~ ., data = abalone_no_sex, method = \"gbm\", trControl = train_control, verbose = FALSE)\n\n\nLinear Regression: Linear regression model is trained, which is useful as a baseline to assess the performance of more complex models.\n\n\nCode\n# Linear Regression\nmodels$lm &lt;- train(Rings ~ ., data = abalone_no_sex, method = \"lm\", trControl = train_control)\n\n\n\n\nCode\npredictions_rf &lt;- predict(models$rf, test_data)\npredictions_svm &lt;- predict(models$svm, test_data)\npredictions_gbm &lt;- predict(models$gbm, test_data)\npredictions_lm &lt;- predict(models$lm, test_data)\n\nrmse_rf &lt;- RMSE(predictions_rf, test_data$Rings)\nr2_rf &lt;- R2(predictions_rf, test_data$Rings)\n\nrmse_svm &lt;- RMSE(predictions_svm, test_data$Rings)\nr2_svm &lt;- R2(predictions_svm, test_data$Rings)\n\nrmse_gbm &lt;- RMSE(predictions_gbm, test_data$Rings)\nr2_gbm &lt;- R2(predictions_gbm, test_data$Rings)\n\nrmse_lm &lt;- RMSE(predictions_lm, test_data$Rings)\nr2_lm &lt;- R2(predictions_lm, test_data$Rings)\n\ncat(\"RF RMSE:\", rmse_rf, \"R2:\", r2_rf, \"\\n\")\n\n\nRF RMSE: 0.974506 R2: 0.9249036 \n\n\nCode\ncat(\"SVM RMSE:\", rmse_svm, \"R2:\", r2_svm, \"\\n\")\n\n\nSVM RMSE: 1.969508 R2: 0.6417577 \n\n\nCode\ncat(\"GBM RMSE:\", rmse_gbm, \"R2:\", r2_gbm, \"\\n\")\n\n\nGBM RMSE: 1.893171 R2: 0.6613736 \n\n\nCode\ncat(\"LM RMSE:\", rmse_lm, \"R2:\", r2_lm, \"\\n\")\n\n\nLM RMSE: 2.125251 R2: 0.5691083 \n\n\nRandom Forest shows the best performance among all models tested, with the lowest RMSE and highest RÂ². This suggests it is most effective at capturing the variance in the data and making accurate predictions about the age of abalones.Based on the measurements, the Random Forest model is the clear winner and the best option for this specific forecasting task. Its low RMSE shows that its predictions are near to real values, and its strong RÂ² value suggests that it can explain a considerable percentage of the variance in the age of abalones.\n\n\nSetting up Training control:\nThis methodically investigate several parameter choices to get the optimal option that reduces the root mean square error (RMSE) while estimating the number of rings in abalones. In addition to trying to increase accuracy, the model tuning makes sure the model performs well when applied to previously unknown data by employing a grid search with cross-validation. This methodical technique aids in choosing the best model settings to successfully improve predictive performance.\n\n\nCode\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, search = \"grid\")\n\ntune_grid &lt;- expand.grid(\n  mtry = seq(2, ncol(abalone_no_sex) - 1, by = 1), \n  splitrule = c(\"variance\"),\n  min.node.size = c(5, 10)\n)\n\nset.seed(12)\ntuned_model &lt;- train(Rings ~ ., data = abalone_no_sex, method = \"ranger\",\n                     trControl = train_control, tuneGrid = tune_grid,\n                     metric = \"RMSE\")\n\n\nGrowing trees.. Progress: 59%. Estimated remaining time: 31 seconds.\n\n\n\n\nImplementing Principal Component Analysis:\nBy using PCA, you reduce noise and computational load, potentially improving the modelsâ€™ performances. Training multiple models allows for a comparative analysis to determine which model best handles the PCA-transformed data in predicting abalone age.\n\n\nCode\nlibrary(caret)\nlibrary(FactoMineR)\n\n#abalone_no_sex =abalone[, !names(abalone) %in% c(\"Sex\")]\nabalone_pca = abalone_no_sex[, !names(abalone) %in% c(\"Rings\")]\n\ndata = abalone_pca  \ndata_scaled = scale(data)\n\npca_result = PCA(data_scaled, graph = FALSE)\n\nnum_components = which(cumsum(pca_result$eig[,2]) &gt; 85)[1]\ndata_pca = pca_result$ind$coord[, 1:num_components]\nnum_components = which(cumsum(pca_result$eig[,2]) &gt; 85)[1]\ndata_pca = pca_result$ind$coord[, 1:num_components]\n\n\nfinal_data = data.frame(Rings = abalone_pca$Rings, data_pca)\n\nset.seed(12)\nindex = createDataPartition(final_data$Rings, p = 0.8, list = TRUE)\ntrain_data_pca = final_data[index$Resample1,]\ntest_data_pca = final_data[-index$Resample1,]\n\ntrain_control_pca = trainControl(method = \"cv\", number = 10)\nmodels_pca = list()\nmodels_pca$rf = train(Rings ~ ., data = train_data_pca, method = \"rf\", trControl = train_control_pca)\n\n\nnote: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .\n\n\nCode\nmodels_pca$svm = train(Rings ~ ., data = train_data_pca, method = \"svmRadial\", trControl = train_control_pca)\nmodels_pca$gbm = train(Rings ~ ., data = train_data_pca, method = \"gbm\", trControl = train_control_pca, verbose = FALSE)\n\n# Linear Regression\nmodels_pca$lm &lt;- train(Rings ~ ., data = train_data_pca, method = \"lm\", trControl = train_control_pca)\n\npredictions_rf_pca = predict(models_pca$rf, test_data_pca)\npredictions_svm_pca = predict(models_pca$svm, test_data_pca)\npredictions_gbm_pca = predict(models_pca$gbm, test_data_pca)\npredictions_lm_pca = predict(models_pca$lm, test_data_pca)\n\nrmse_rf_pca = RMSE(predictions_rf_pca, test_data_pca$Rings)\nr2_rf_pca = R2(predictions_rf_pca, test_data_pca$Rings)\n\nrmse_svm_pca = RMSE(predictions_svm_pca, test_data_pca$Rings)\nr2_svm_pca = R2(predictions_svm_pca, test_data_pca$Rings)\n\nrmse_gbm_pca = RMSE(predictions_gbm_pca, test_data_pca$Rings)\nr2_gbm_pca = R2(predictions_gbm_pca, test_data_pca$Rings)\n\nrmse_lm_pca = RMSE(predictions_lm_pca, test_data_pca$Rings)\nr2_lm_pca = R2(predictions_lm_pca, test_data_pca$Rings)\n\ncat(\"RF RMSE:\", rmse_rf_pca, \"R2:\", r2_rf_pca, \"\\n\")\n\n\nRF RMSE: 0.4754979 R2: 0.9781475 \n\n\nCode\ncat(\"SVM RMSE:\", rmse_svm_pca, \"R2:\", r2_svm_pca, \"\\n\")\n\n\nSVM RMSE: 0.7983523 R2: 0.9387943 \n\n\nCode\ncat(\"GBM RMSE:\", rmse_gbm_pca, \"R2:\", r2_gbm_pca, \"\\n\")\n\n\nGBM RMSE: 0.5245579 R2: 0.9734159 \n\n\nCode\ncat(\"LM RMSE:\", rmse_lm_pca, \"R2:\", r2_lm_pca, \"\\n\")\n\n\nLM RMSE: 0.5061476 R2: 0.9754974 \n\n\nOn this changed dataset, Random Forest is clearly the best performer, but Linear Regression also demonstrates excellent performance. By separating the most important characteristics and minimizing noise and duplication in the data, the application of PCA prior to model training has probably improved performance. In practical settings, these modelsâ€”Random Forest and Linear Regression in particularâ€”would be very useful for age prediction based on physical attributes, such as in biological research or abalone resource management.\n\n\nImplementation of the Random Forest model in Shiny:\n#Shiny\n\n\nClassification and Prediction of Maturity of Abalone:\nAbalones are classified into Mature and Immature based on the count of their Rings. It involves determining whether the abalone has reached a biological threshold that marks it as mature or immature. This method relies on analyzing the ring count, which correlates with age, as each ring generally signifies one year of growth.\n\nUsing the training data as a Logistic Regression model, all other variables are used by the model to forecast the â€˜Matureâ€™ status; however, â€˜Ringsâ€™ is specifically left out since â€˜Matureâ€™ is derived directly from it. The dependent variable (logistic regression) is binary, according to the binomial family specification. With logistic regression, data is fitted to a logistic curve to determine the likelihood that an event will occur. The values it gives, which range from 0 to 1, are understood as the likelihood that a certain observation falls into the category designated as â€œ1.â€ This makes it perfect for situations when you want to determine the likelihood of an outcome in addition to predicting a binary outcome.Since the maturity status is a binary variable, logistic regression is the obvious choice.\n\n\nCode\nlibrary(dplyr)\nlibrary(caret)  \nlibrary(pROC)   \n\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n\nCode\nabalone$Mature &lt;- if_else(abalone$Rings &gt;= 10, \"mature\", \"immature\")\n\nabalone$Sex &lt;- as.factor(abalone$Sex)\n\nset.seed(12)\nindex &lt;- createDataPartition(abalone$Mature, p = 0.8, list = TRUE)\ntrain_data &lt;- abalone[index[[1]], ]\ntest_data &lt;- abalone[-index[[1]], ]\n\ntrain_data$Mature &lt;- as.integer(train_data$Mature == \"mature\")\n\nmodel_log &lt;- glm(Mature ~ . -Rings, data = train_data, family = binomial())\n\npredictions_log &lt;- predict(model_log, test_data, type = \"response\")\npredicted_classes_log &lt;- if_else(predictions_log &gt; 0.5, \"mature\", \"immature\")\npredicted_classes_log &lt;- factor(predicted_classes_log, levels = c(\"immature\", \"mature\"))\ntest_data$Mature &lt;- factor(test_data$Mature, levels = c(\"immature\", \"mature\"))\n\nconfusion_matrix &lt;- confusionMatrix(predicted_classes_log, test_data$Mature)\n\nroc_curve &lt;- roc(test_data$Mature, predictions_log)\n\n\nSetting levels: control = immature, case = mature\n\n\nSetting direction: controls &lt; cases\n\n\nCode\nplot(roc_curve, main = \"ROC Curve for Logistic Regression\")\n\n\n\n\n\n\n\nInterpretation:\nThe ROC (Receiver Operating Characteristic) curve shown for the logistic regression model used to predict the maturity of abalones based on their ring counts provides a visual assessment of the modelâ€™s diagnostic ability. The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) across a range of decision thresholds. A curve that approaches the top left corner of the plot indicates a high level of sensitivity and specificity, demonstrating an excellent predictive performance. In this graph, the curve significantly deviates from the diagonal line, which represents a random guess, suggesting that the logistic regression model has a strong ability to distinguish between mature and immature abalones. The area under the curve (AUC), while not explicitly stated, appears to be substantially higher than 0.5 (the AUC of a random classifier), confirming the effectiveness of the model.\n\n\nPredictive Accuracy of Abalone Age between different Sex categories:\nThe dataset is divided into 3 distinct subsets - Male, Female and Infant. Here, we are creating DataPartition() and each subset is further split into Training and Testing sets. Following this, Random Forest models are constructed independently for every Sex category, with the Sex. varibale being eliminated as a predictor. The glimpse(abalone) function call is likely used for a quick overview of the abalone dataset to understand its structure and variables.\nThe performance metrics for each model are printed, showing the RMSE and RÂ² values for males, females, and infants. These metrics help in comparing the accuracy and explanatory power of the models across different sex categories, highlighting potential differences in prediction reliability and effectiveness.\n\n\nCode\nglimpse(abalone)\n\n\nRows: 4,177\nColumns: 10\n$ Sex            &lt;fct&gt; M, M, F, M, I, I, F, F, M, F, F, M, M, F, F, M, I, F, Mâ€¦\n$ Length         &lt;dbl&gt; 0.455, 0.350, 0.530, 0.440, 0.330, 0.425, 0.530, 0.545,â€¦\n$ Diameter       &lt;dbl&gt; 0.365, 0.265, 0.420, 0.365, 0.255, 0.300, 0.415, 0.425,â€¦\n$ Height         &lt;dbl&gt; 0.095, 0.090, 0.135, 0.125, 0.080, 0.095, 0.150, 0.125,â€¦\n$ Whole_weight   &lt;dbl&gt; 0.5140, 0.2255, 0.6770, 0.5160, 0.2050, 0.3515, 0.7775,â€¦\n$ Shucked_weight &lt;dbl&gt; 0.2245, 0.0995, 0.2565, 0.2155, 0.0895, 0.1410, 0.2370,â€¦\n$ Viscera_weight &lt;dbl&gt; 0.1010, 0.0485, 0.1415, 0.1140, 0.0395, 0.0775, 0.1415,â€¦\n$ Shell_weight   &lt;dbl&gt; 0.150, 0.070, 0.210, 0.155, 0.055, 0.120, 0.330, 0.260,â€¦\n$ Rings          &lt;int&gt; 15, 7, 9, 10, 7, 8, 20, 16, 9, 19, 14, 10, 11, 10, 10, â€¦\n$ Mature         &lt;chr&gt; \"mature\", \"immature\", \"immature\", \"mature\", \"immature\",â€¦\n\n\nCode\nabalone_m &lt;- abalone %&gt;% filter(Sex == 'M')\nabalone_f &lt;- abalone %&gt;% filter(Sex == 'F')\nabalone_i &lt;- abalone %&gt;% filter(Sex == 'I')\n\nlibrary(caret)\n\nset.seed(123)  \ntrainIndex_m &lt;- createDataPartition(abalone_m$Rings, p = 0.8, list = FALSE)\ntrain_m &lt;- abalone_m[trainIndex_m, ]\ntest_m &lt;- abalone_m[-trainIndex_m, ]\n\ntrainIndex_f &lt;- createDataPartition(abalone_f$Rings, p = 0.8, list = FALSE)\ntrain_f &lt;- abalone_f[trainIndex_f, ]\ntest_f &lt;- abalone_f[-trainIndex_f, ]\n\ntrainIndex_i &lt;- createDataPartition(abalone_i$Rings, p = 0.8, list = FALSE)\ntrain_i &lt;- abalone_i[trainIndex_i, ]\ntest_i &lt;- abalone_i[-trainIndex_i, ]\n\n\nlibrary(randomForest)\n\nmodel_m &lt;- randomForest(Rings ~ . -Sex, data = train_m)\nmodel_f &lt;- randomForest(Rings ~ . -Sex, data = train_f)\nmodel_i &lt;- randomForest(Rings ~ . -Sex, data = train_i)\n\n\n\npredictions_m &lt;- predict(model_m, test_m)\nrmse_m &lt;- sqrt(mean((predictions_m - test_m$Rings)^2))\nr2_m &lt;- cor(predictions_m, test_m$Rings)^2\n\npredictions_f &lt;- predict(model_f, test_f)\nrmse_f &lt;- sqrt(mean((predictions_f - test_f$Rings)^2))\nr2_f &lt;- cor(predictions_f, test_f$Rings)^2\n\npredictions_i &lt;- predict(model_i, test_i)\nrmse_i &lt;- sqrt(mean((predictions_i - test_i$Rings)^2))\nr2_i &lt;- cor(predictions_i, test_i$Rings)^2\n\ncat(\"Male RMSE:\", rmse_m, \"RÂ²:\", r2_m, \"\\n\")\n\n\nMale RMSE: 1.938495 RÂ²: 0.5603675 \n\n\nCode\ncat(\"Female RMSE:\", rmse_f, \"RÂ²:\", r2_f, \"\\n\")\n\n\nFemale RMSE: 2.333701 RÂ²: 0.4842709 \n\n\nCode\ncat(\"Infant RMSE:\", rmse_i, \"RÂ²:\", r2_i, \"\\n\")\n\n\nInfant RMSE: 1.337307 RÂ²: 0.7408071 \n\n\n\n\nInterpretation:\nThe models perform best for infant abalones, indicating that the features selected for prediction are more predictive for this group. In contrast, the models for male and especially female abalones show lower accuracy and fit, suggesting potential complexities or variations in growth patterns not as effectively captured by the model. This analysis highlights the need for potentially refining the models or exploring additional features that could better account for the variations in ring counts, particularly for female abalones.\n\n\nRelation between Rings and Predictors:\nPredictor transformations are techniques used to modify the original predictor variables to enhance the performance and interpretability of models. These transformations are crucial for addressing issues such as non-linearity, non-normality, heteroscedasticity (non-constant variance), and to better capture the relationships between predictors and the response variable.\n\n\nCode\nlibrary(gridExtra)\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:randomForest':\n\n    combine\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nCode\nlibrary(tidyverse)\nset.seed(42)\nindexes &lt;- sample(1:nrow(abalone), size = 0.3 * nrow(abalone))\nabalone_train &lt;- abalone[-indexes,]\nabalone_test &lt;- abalone[indexes,]\npar(mfrow = c(3, 3),oma=c(0,0,0,0))\ncreate_scatter_plot &lt;- function(xvar,yvar,xtitle,ytitle) {\n  ggplot(abalone_train,aes(x=xvar,y=yvar, col=Sex)) + \n    geom_point() + \n    geom_jitter() + \n    xlab(paste(xtitle)) +  \n    ylab(paste(ytitle))\n}\ngrid.arrange(\ncreate_scatter_plot(abalone_train$Length,abalone_train$Rings,\"Length\",\"Rings\"),\ncreate_scatter_plot(abalone_train$Diameter,abalone_train$Rings,\"Diameter\",\"Rings\"),\ncreate_scatter_plot(abalone_train$Height,abalone_train$Rings,\"Height\",\"Rings\"),\ncreate_scatter_plot(abalone_train$Whole_weight,abalone_train$Rings,\"Whole_weight\",\"Rings\"),\ncreate_scatter_plot(abalone_train$Shell_weight,abalone_train$Rings,\"Shell_weight\",\"Rings\"),\ncreate_scatter_plot(abalone_train$Shucked_weight,abalone_train$Rings,\"Shucked_weight\",\"Rings\"),\ncreate_scatter_plot(abalone_train$Viscera_weight,abalone_train$Rings,\"Viscera_weight\",\"Rings\")\n)\n\n\n\n\n\n\n\nInterpretation:\nThe scatter plots provided display various physical measurements against the number of rings in abalones, categorized by sex. These plots suggest potential non-linear relationships and variable scales, indicating that transformations such as logarithmic, polynomial, or standardizing might enhance model accuracy by normalizing distributions and reducing skewness. The evident differences across sexes also hint that interactions between predictors and sex could be significant, potentially necessitating separate models or tailored transformations for each sex group. Implementing such transformations could address the non-linear patterns, manage the influence of outliers, and standardize predictor scales, thereby improving the interpretability and performance of predictive models for estimating abalone age.\n\n\nAdvantages:\n\nHandling Non-Linearity: Random forests effectively manage non-linear relationships between variables without the need for transformations, making them ideal for complex ecological data like that of abalones.\nFeature Importance: This method provides valuable insights into which features (e.g., shell weight, diameter) are most predictive of age, aiding in understanding biological growth patterns.\nRobust to Overfitting: Due to the ensemble nature of random forests, where multiple trees vote on the outcome, the model is generally more robust to overfitting compared to single decision trees.\nAccommodating Missing Data: Random forests can handle missing values in the data without requiring explicit imputation, which can simplify preprocessing steps.\nModel Flexibility: They can be used for both classification and regression tasks, providing flexibility in terms of application.\n\n\n\nDisadvantages:\n\nModel Complexity and Interpretability: Random forests involve numerous decision trees, which can make the model complex and difficult to interpret compared to simpler, linear models.\nComputationally Intensive: Training a random forest can be computationally expensive, especially with a large number of trees and deep tree structures.\nLess Effective for Extrapolation: They do not perform well in cases where predictions need to be made outside the range of the training dataâ€™s features.\n\n\n\nLimitations:\n\nHyperparameter Tuning: Determining the optimal number of trees, depth of each tree, and other parameters can be time-consuming and requires a methodical approach to tuning.\nData Imbalance: If the dataset is imbalanced, random forest models can be biased towards the majority class, especially in classification tasks.\nHigh Dimensionality: While random forests handle multiple features well, extremely high-dimensional spaces (many features) can lead to longer training times and might require dimensionality reduction techniques to ensure model efficiency.\nNo Performance Guarantee: As with any machine learning model, there is no guarantee that a random forest will perform well on all types of data, and its success heavily depends on the nature of the dataset and how representative the training data is of the broader population of abalones.\n\n\n\nConclusion:\nAs a summary to the Research questions in report,\nAccording to the analysis and interpretation when the physical measurements are high i.e., when the length, diameter and weight is a greater number then the age of the abalone is also older.Random Forest and Logistic Regression models were employed to predict abalone age, defined by the number of rings (age = rings + 1.5 years). Predictive models used physical measurements like shell length, diameter, and weight as inputs.\nThe age of abalone was predicted by comparing various different models such as SVM, GBM, random forest and Linear Regression, out of which Random forest gave the best accuracy of 97% after PCA data was used for the model.\nRandom Forest models were trained separately for male, female, and infant abalones. The results indicated differences in predictive accuracy.findings suggest significant differences in how well physical measurements can predict age across different sex categories. This could be due to intrinsic differences in growth patterns or the effectiveness of the used features in capturing underlying patterns for each category.Infant Abalones exhibited the best predictive accuracy with an RMSE of 1.765061 and an RÂ² of 0.5488142, suggesting that age prediction is more straightforward or consistent in younger abalones, or the features used are more predictive for this group.\n\n\nReferences:\n[1] UCI Machine Learning Repository. (n.d.).Â https://archive.ics.uci.edu/dataset/1/abalone\n[2] Allhan Uysal and Altay Guvenir. Instance-Based Regression by Partitioning Feature projections. Applied.2004.\n[3] Lin, A., & Meyers, M. A. (2005), Growth and structure in abalone shell. Materials Science and Engineering: A, 390(1-2), 27-41"
  },
  {
    "objectID": "workhistory.html",
    "href": "workhistory.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Infosys - SAP S&M Analyst Hyderabad, India\nâ€¢ Duration: 2 years 10 months\nBharat Heavy Electricals Limited - Summer Intern Hyderabad, India\nâ€¢ Duration: 1 month"
  }
]